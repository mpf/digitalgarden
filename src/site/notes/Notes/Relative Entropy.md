---
{"dg-publish":true,"permalink":"/notes/relative-entropy/"}
---


> This note describes how to the relative entropy function (also known as Kullback-Leibler (KL) divergence) can be reparameterixed with the soft-max function, thereby enlarging its domain from the simplex into the full space while still preserving convexity.
>
> See also see [[Notes/Bregman Divergence\|Bregman Divergence]].

## Relative entropy

For a finite probability space $\Omega$, a probability mass function $p:\Omega\to[0,1]$ satisfies the requirement $\sum_{x\in\Omega}p(x)=1$. The Kullbach-Leibler (KL) divergence between any two mass probability functions $p, q\in\Delta_\Omega$ is
$$
H_{r}(p,q) := \sum_{x\in\Omega}p(x)\log \frac{p(x)}{q(x)}
$$
Note that $\mathrm{dom} H_{r}=\left( \Delta_\Omega\times\text{int}\Delta_\Omega \right)$. Implicit in this definition seems to also be the requirement that $p(x)\le q(x)$ for all $x\in\Omega$. 

### Relative entropy is convex

Relative entropy is a Bregman divergence generated by the negative entropy function
$$
H(p) = \sum_{x\in\Omega}p(x)\log p(x). 
$$
Thus $H_{r}(p,q) = D_H(p,q)=H(p) - H(q) - \langle \nabla H(q),p-q\rangle$.

Relative entropy is convex in _both_ arguments, which is [[Notes/Bregman Divergence#Convexity\|not necessarily true of all Bregman divergence]] functions. To establish the convexity of $H_{r}$ in both arguments, it's sufficient to establish that the scalar function $h(\nu,\mu)=\nu\log(\nu/\mu)$ is convex. The gradient and Hessian are
$$
\begin{align}
\nabla h(\nu,\mu) &=  \begin{pmatrix}
1 + \log(\nu / \mu) \\ -\nu/\mu
\end{pmatrix}
\end{align}
\quad\text{and}\quad
\nabla^2 h(\nu,\mu) = \begin{pmatrix}
1 / \nu & -1 / \mu \\ -1 / \mu & \nu / \mu^2
\end{pmatrix}
$$
It's straightforward to verify that the Hessian is positive semidefinite, and hence $h$ and thus $H_r$ is convex in both arguments.

## Soft-max reparameterization

The soft-max reparameterization is based the dual represention of the relative entropy function. Because relative entropy is of the Legendre type, it holds that $\nabla H \circ \nabla  H^*=\text{Id}$, where
$$
H^*:\mathbb{R}^{|\Omega|}\to \mathbb{R}:q^{*}\mapsto \log \sum_{x\in\Omega}\exp q^{*}(x)
$$
and
$$
\nabla H^*:\mathbb{R}^{|\Omega|}\to \Delta_{\Omega}: q^{*} \mapsto \left[ \frac{\exp q^{*}(x)}{\sum_{x'\in\Omega} \exp q^{*}(x')} \right]_{x\in\Omega}.
$$

In particular, we use the dual pair of identities:
$$
\begin{align}
D_{H}(p,q) &= D_{H^{*}}(\nabla H(q),\nabla H(p)),\\
D_{H^{*}}(q^{*},p^{*}) &= D_{H}(\nabla H^{*}(p^{*}),\nabla H^{*}(q^{*})).
\end{align}
$$
Fix a target distribution $q\in\Delta_{\Omega}$ and some appropriate regularization function $\phi:\mathbb{R}^{|\Omega|}\to \mathbb{R}$. It follows from the above identities that
$$
\begin{align}
H_{r}(p,q) &= D_{H}(p,q) \\
        &= D_{H}(\nabla H^{*}(\bar{p}),q) \quad [\bar{p}:=\nabla H(p)] \\
        &= D_{H^{*}}(\nabla H(q),\nabla H(\nabla H^{*}(\bar{p})) \\
        &= D_{H^{*}}(\nabla H(q),\bar{p})
\end{align}
$$
>[!tip] Convexity of $D_{H^{*}}$?
>I think that in order to establish convexity of the above with respect to $\bar{p}$ we would need to establish convexity of $D_{H^{*}}$ with respect to the second argument.




