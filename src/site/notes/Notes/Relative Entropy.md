---
{"dg-publish":true,"permalink":"/notes/relative-entropy/"}
---


> see [[Notes/Bregman Divergence\|Bregman Divergence]]

This note describes how to optimize the Kullback-Leibler (KL) divergence using a soft-max parameterization that cases the elliminates the need to explicitly enforce a simplex constraint.

## Relative entropy

For a finite probability space $\Omega$, a probability mass function $p:\Omega-\to[0,1]$ satisfies the requirement $\sum_{x\in\Omega}p(x)=1$.

The Kullbach-Leibler (KL) divergence between any two mass functions $p, q\in\Delta_\Omega$ is
$$
\KL(p,q) = \sum_{x\in\Omega}p(x)\log \frac{p(x)}{q(x)}.
$$
Implicit in this definition is that $q$ must always dominate $p$, and so the domain of $\KL$ is $(\Delta_\Omega\times\Delta_\Omega)\cap\{(p,q)\mid p(x)\le q(x)\ \forall x\}$. 

### Convexity

Relative entropy is a Bregman divergence generated by the negative entropy of a mass $p$,
$$
H(p) = \sum_{x\in\Omega}p(x)\log p(x). 
$$
Thus $\KL(p,q) = D_H(p,q)=H(p) - H(q) - \ip{\nabla H(q)}{p-q}$.

Relative entropy is convex in _both_ arguments, which is [[Notes/Bregman Divergence#Convexity\|not necessarily true of a Bregman divergence]] with an arbitrary generator. To determine the convexity of $\KL$ in both arguments, it's sufficient to establish that the scalar function $h(\nu,\mu)=\nu\log(\nu/\mu)$ is convex. The gradient and Hessian are
$$
\begin{align}
\nabla h(\nu,\mu) &=  \begin{pmatrix}
1 + \log(\nu / \mu) \\ -\nu/\mu
\end{pmatrix}
\end{align}
\quad\text{and}\quad
\nabla^2 h(\nu,\mu) = \begin{pmatrix}
1 / \nu & -1 / \mu \\ -1 / \mu & \nu / \mu^2
\end{pmatrix}
$$
It is straightforward to verify that the Hessian is positive semidefinite, and hence $h$ is convex in both arguments.

